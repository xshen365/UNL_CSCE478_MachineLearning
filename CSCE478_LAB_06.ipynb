{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Ordinary Least Squares (OLS) Method \n",
    "\n",
    "We will perform linear regression using\n",
    "- Scikit Learn's OLS model\n",
    "- Manually coded OLS method\n",
    "\n",
    "\n",
    "The sklearn OLS implementation code is given in this notebook. You will have to implement the OLS method manually on the given dataset (OLS_Data.csv).\n",
    "\n",
    "\n",
    "### OLS\n",
    "\n",
    "OLS is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function.\n",
    "\n",
    "OLS finds the optimal parameters by computing a closed-form solution for the **Normal equation**.\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/linear_model.html#linear-model\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will use a dataset (OLS_Data.csv) containing 14 variables (14 dimensional feature)\n",
    "\n",
    "Input variables:\n",
    "X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14\n",
    "\n",
    "Output variable: \n",
    "y\n",
    "\n",
    "### Note:\n",
    "This dataset might have colinearity in the input variables resulting into the singularity problem. It might cause the OLS method not working. You may need to fix the singularity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: OLS Linear Regression Using Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv, det, matrix_rank\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc.\n",
    "\n",
    "Download the \"OLS_Data.csv\" file to load data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the csv file as a Pandas DataFrame object denoted as \"df\"\n",
    "df = pd.read_csv('OLS_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1    X2    X3  X4     X5     X6    X7      X8  X9  X10   X11     X12  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296  15.3  396.90   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242  17.8  396.90   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242  17.8  392.83   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222  18.7  394.63   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222  18.7  396.90   \n",
       "\n",
       "    X13      X14     y  \n",
       "0  4.98  0.00632  24.0  \n",
       "1  9.14  0.02731  21.6  \n",
       "2  4.03  0.02729  34.7  \n",
       "3  2.94  0.03237  33.4  \n",
       "4  5.33  0.06905  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 15 columns):\n",
      "X1     506 non-null float64\n",
      "X2     506 non-null float64\n",
      "X3     506 non-null float64\n",
      "X4     506 non-null int64\n",
      "X5     506 non-null float64\n",
      "X6     506 non-null float64\n",
      "X7     506 non-null float64\n",
      "X8     506 non-null float64\n",
      "X9     506 non-null int64\n",
      "X10    506 non-null int64\n",
      "X11    506 non-null float64\n",
      "X12    506 non-null float64\n",
      "X13    506 non-null float64\n",
      "X14    506 non-null float64\n",
      "y      506 non-null float64\n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 59.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix: Feature Correlations\n",
    "\n",
    "Check if the data matrix has colinearity (1 or close to 1) in its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "X1   1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
      "X2  -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
      "X3   0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
      "X4  -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
      "X5   0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
      "X6  -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
      "X7   0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
      "X8  -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
      "X9   0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
      "X10  0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
      "X11  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
      "X12 -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
      "X13  0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
      "X14  1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
      "y   -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
      "\n",
      "           X8        X9       X10       X11       X12       X13       X14  \\\n",
      "X1  -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  1.000000   \n",
      "X2   0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995 -0.200469   \n",
      "X3  -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  0.406583   \n",
      "X4  -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929 -0.055892   \n",
      "X5  -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  0.420972   \n",
      "X6   0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808 -0.219247   \n",
      "X7  -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  0.352734   \n",
      "X8   1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996 -0.379670   \n",
      "X9  -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  0.625505   \n",
      "X10 -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  0.582764   \n",
      "X11 -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  0.289946   \n",
      "X12  0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087 -0.385064   \n",
      "X13 -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  0.455621   \n",
      "X14 -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621  1.000000   \n",
      "y    0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663 -0.388305   \n",
      "\n",
      "            y  \n",
      "X1  -0.388305  \n",
      "X2   0.360445  \n",
      "X3  -0.483725  \n",
      "X4   0.175260  \n",
      "X5  -0.427321  \n",
      "X6   0.695360  \n",
      "X7  -0.376955  \n",
      "X8   0.249929  \n",
      "X9  -0.381626  \n",
      "X10 -0.468536  \n",
      "X11 -0.507787  \n",
      "X12  0.333461  \n",
      "X13 -0.737663  \n",
      "X14 -0.388305  \n",
      "y    1.000000  \n",
      "\n",
      "The columns that have high correlation are:  ('X14', 'X1')\n"
     ]
    }
   ],
   "source": [
    "print(df.corr())\n",
    "\n",
    "high_corr_cols = None\n",
    "for i in df.corr():\n",
    "    for j in df.corr():\n",
    "        if i != j and df.corr()[i][j] == 1:\n",
    "            high_corr_cols = (i, j)\n",
    "print('\\nThe columns that have high correlation are: ', high_corr_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# data matrix X\n",
    "X = df.drop(columns = ['y'])\n",
    "\n",
    "# target vector y\n",
    "y = df['y']\n",
    "\n",
    "#print(X.shape)\n",
    "print(X.shape)\n",
    "#print(y.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization.\n",
    "\n",
    "Use sklearns StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ...  0.44105193 -1.0755623\n",
      "  -0.41978194]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ...  0.44105193 -0.49243937\n",
      "  -0.41733926]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ...  0.39642699 -1.2087274\n",
      "  -0.41734159]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  0.44105193 -0.98304761\n",
      "  -0.41344658]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  0.4032249  -0.86530163\n",
      "  -0.40776407]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  0.44105193 -0.66905833\n",
      "  -0.41500016]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_scale = scaler.transform(X)\n",
    "print(X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset\n",
    "\n",
    "We use sklearn's train_test_split function to spilt the dataset into training and test subsets. The data is shuffled by default before splitting.\n",
    "\n",
    "This function splits arrays or matrices into **random** train and test subsets.\n",
    "\n",
    "For the **reproducibility of the results**, we need to use the same seed for the random number generator. The seed is set by the \"random_state\" parameter of the split function. \n",
    "\n",
    "It should return the following 4 matrices/vectors.\n",
    "- X_train\n",
    "- y_train\n",
    "- X_test\n",
    "- y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models\n",
    "\n",
    "We will use the following linear regression models.\n",
    "\n",
    "- Ordinary least squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination ($R^2$ or $r^2$)\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "\n",
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "$R^2 = \\frac{Explained Variation}{Total Variation}$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "\n",
    "#### <font color=red>In general, the higher the R-squared, the better the model fits your data.</font>\n",
    "\n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Ordinary Least Squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n",
    "\n",
    "#### Sklearn's OLS model implementation code is given for you to review.\n",
    "\n",
    "Then, you will have to manually code the OLS method.\n",
    "\n",
    "\n",
    "#### <font color=red>The MSE and $r^2$ error values from your manually coded OLS method must match with sklearn LinearRegressor's obtained values.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 22.4852682393169\n",
      "Coefficients: \n",
      " [-0.48574711  0.70155562  0.27675212  0.70653152 -1.99143043  3.11571836\n",
      " -0.17706021 -3.04577065  2.28278471 -1.79260468 -1.97995351  1.12649864\n",
      " -3.62814937 -0.48574711]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 21.64\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Create the sklearn OLS linear regression object\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg.coef_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted = lin_reg.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# To compute \n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % lin_reg.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Sklearn OLS Model Using Test Data \n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "The goal is to see how the model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "y_test_predicted = lin_reg.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Coded OLS Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_train_bias^T.X_train_bias):  0.0\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5fb0b3323508>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Closed form (OLS) solution for weight vector w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nThe weight vector:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Manually code the OLS Method for Linear Regression\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "det_X_train_bias = det(X_train_bias.T.dot(X_train_bias))\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_train_bias^T.X_train_bias): \", det_X_train_bias)\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "z = X_train_bias.T.dot(X_train_bias)\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z).dot(X_train_bias.T).dot(y_train)\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\", w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_train_predicted = X_train_bias.dot(w)\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error: %.2f\" %mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" %r2_score(y_train, y_train_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation on the Performance of the Manually Coded OLS Solution\n",
    "\n",
    "You might get the **Singular matrix** error.\n",
    "\n",
    "The determinant of the $X_{bias}^T.X_{bias}$ should be 0.\n",
    "\n",
    "There must be colinearity in the columns of the data matrix X.\n",
    "\n",
    "Find which columns are collinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying OLS Method on Data Matrix With Collinearity in Columns\n",
    "\n",
    "Solve the singularity problem by adding small positive numbers on the diagonal of the $X_{bias}^T.X_{bias}$ matrix.\n",
    "\n",
    "This regularization technique is known as **Ridge Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_train_bias^T.X_train_bias):  4.645063289219296e+19\n",
      "\n",
      "-------- Fixing the Singularity of (X_bias^T).X_bias ------------\n",
      "\n",
      "The weight vector:\n",
      " [22.48521177 -0.48574245  0.70153482  0.27672332  0.70653526 -1.99139527\n",
      "  3.115727   -0.17706139 -3.04573205  2.28270084 -1.7925275  -1.97994666\n",
      "  1.12649546 -3.62813639 -0.48574245]\n",
      "\n",
      "The weight vector after changed diagonal(big weights):\n",
      " [ 0.09170736 -0.01437852  0.01201041 -0.01931415  0.0082001  -0.01399431\n",
      "  0.03071478 -0.01237209  0.00908906 -0.01593964 -0.0188475  -0.02447065\n",
      "  0.01212677 -0.0296035  -0.01437852]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error:21.64\n",
      "\n",
      "Mean squared error of changed weights: 600.08\n",
      "\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n",
      "\n",
      "Coefficient of determination r^2 variance score of changed weights[1 is perfect prediction]: -5.91\n"
     ]
    }
   ],
   "source": [
    "# Bayesian (Regularized) OLS Method for Linear Regression: Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_train_bias^T.X_train_bias): \", det(X_train_bias.T.dot(X_train_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "z = X_train_bias.T.dot(X_train_bias)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X_bias^T).X_bias ------------\")\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z; name the matrix as \"diagonal\"\n",
    "diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "# Add small positive non-zero numbers on the diagonal\n",
    "diagonal_small = diagonal*0.001\n",
    "diagonal_big = diagonal*100000\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z+diagonal_small).dot(X_train_bias.T).dot(y_train)\n",
    "w_big = np.linalg.inv(z+diagonal_big).dot(X_train_bias.T).dot(y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\", w)\n",
    "print(\"\\nThe weight vector after changed diagonal(big weights):\\n\", w_big)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_train_predicted = X_train_bias.dot(w)\n",
    "y_train_predicted_big_weight = X_train_bias.dot(w_big)\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error:%.2f\" %mean_squared_error(y_train, y_train_predicted))\n",
    "print(\"\\nMean squared error of changed weights: %.2f\" %mean_squared_error(y_train, y_train_predicted_big_weight))\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"\\nCoefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" %r2_score(y_train, y_train_predicted))\n",
    "print(\"\\nCoefficient of determination r^2 variance score of changed weights[1 is perfect prediction]: %.2f\" %r2_score(y_train, y_train_predicted_big_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data - OLS Linear Regression\n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "Compute the MSE and $r^2$ score using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the test set is: 24.29\n",
      "\n",
      "The R^2 score of the test set is: 0.67\n"
     ]
    }
   ],
   "source": [
    "X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "#z = X_test_bias.T.dot(X_test_bias)\n",
    "#diagonal = np.eye(z.shape[0], dtype=float)\n",
    "#diagonal = diagonal*0.001\n",
    "#w = np.linalg.inv(z+diagonal).dot(X_test_bias.T).dot(y_test)\n",
    "y_test_predicted = X_test_bias.dot(w)\n",
    "print('The MSE of the test set is: %.2f' %mean_squared_error(y_test, y_test_predicted))\n",
    "print('\\nThe R^2 score of the test set is: %.2f' %r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Understanding the Singularity Issue and its Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why do you think the \"singular matrix\" error occured while using OLS method on the “OLS_Data.csv” dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Because there are features(columns) highly correlated with each other(Linearly dependent systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To fix the singularity problem of the $X_{bias}^T.X_{bias}$ matrix what non-zero positive number did you add on its diagonal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Add 0.001 on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Add 100,000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix and report the $MSE$ and the $r^2$ values for the training data set. Explain these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: MSE: 600.08; r^2: -5.91. The mean squared error increased because for each sample the average dispersion from the true values increases. Only half(0.5) of the variance is explained by the model since the diagonal has been changed dramatically and the correlation between features dropped, and thus the R^2(correlation^2) decreased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) After adding 100,000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix what change did you notice in the weights of the model? Did the weights increase/decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The intercept decreased. Some of the columns of the weight vector increased while some decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
